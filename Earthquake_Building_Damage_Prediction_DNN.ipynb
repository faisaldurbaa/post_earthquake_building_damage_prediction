{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n5bBLLNOoMQt",
        "N220rS7Efjrc",
        "ag68wIDACYd_",
        "0K4D_12HWmlv",
        "5CsV-dJTkIve",
        "gJzO-7-xoaI8",
        "HXKYj6D9vSaF",
        "tWIJRu92vbF2",
        "gbyjDI155HIZ",
        "xgw3hW2lfyld",
        "9G4_Y4lllHGg",
        "wXtudo3Mmtl2",
        "l9IUX6RUosbm",
        "ZYmTbIuOqWvl",
        "7OKVN0eMr0u3",
        "OiSAAu7rs1_X",
        "2ilOaa31D8nO"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Post-Earthquake Building Damage Prediction with Nepal 2015 Dataset**\n",
        "[Medium Article](https://medium.com/@durbaafaisal/post-earthquake-building-damage-prediction-with-nepal-2015-dataset-48478f1394c6)"
      ],
      "metadata": {
        "id": "n5bBLLNOoMQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX0-aA6Jnwsk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "!pip install --quiet keras_visualizer\n",
        "from keras_visualizer import visualizer\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jsO309hqn9ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "\n",
        "path = '/path_to_files/'\n",
        "train_values_path = os.path.join(path, 'train_values.csv')\n",
        "train_labels_path = os.path.join(path, 'train_labels.csv')\n",
        "train_values = pd.read_csv(train_values_path, index_col='building_id')\n",
        "train_labels = pd.read_csv(train_labels_path, index_col='building_id')\n",
        "df = train_values.join(train_labels)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PJYA8LBpoHLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "N220rS7Efjrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "QrIIAVW6eQfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "GLOPmrY4hOWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "nVZ40ggJfBYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Identifying geographic location class number\n",
        "\n",
        "print(len(df['geo_level_3_id'].unique()))\n",
        "print(len(df['geo_level_2_id'].unique()))\n",
        "print(len(df['geo_level_1_id'].unique()))"
      ],
      "metadata": {
        "id": "W3TqA430fUjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_counts = df.isnull().sum()\n",
        "missing_percentages = (missing_counts / len(df)) * 100\n",
        "\n",
        "print(missing_counts, missing_percentages)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JVJBPRYphm3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Numerical Feature Overview\n",
        "TARGET_COLUMN = 'damage_grade'\n",
        "\n",
        "print(f\"\\n--- Analysis of Target Variable: {TARGET_COLUMN} ---\")\n",
        "\n",
        "print(\"\\nValue Counts:\")\n",
        "print(df[TARGET_COLUMN].value_counts())\n",
        "\n",
        "print(\"\\nPercentage Distribution:\")\n",
        "print(df[TARGET_COLUMN].value_counts(normalize=True).map('{:.2%}'.format))\n",
        "\n",
        "# Visualize the distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x=TARGET_COLUMN, data=df, palette='viridis', order=df[TARGET_COLUMN].value_counts().index)\n",
        "plt.title('Distribution of Damage Grades')\n",
        "plt.xlabel('Damage Grade')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZSGTZSu-h-5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Remove target variable and columns we might treat as categorical (like IDs or binary flags)\n",
        "exclude = [TARGET_COLUMN]\n",
        "exclude.extend([col for col in df.columns if col.startswith('geo_level')])\n",
        "exclude.extend([col for col in df.columns if col.startswith('has_')])\n",
        "\n",
        "numerical_cols_for_plots = [col for col in numerical_cols if col not in exclude]\n",
        "\n",
        "print(f\"\\n--- Plotting Distributions for {len(numerical_cols_for_plots)} Numerical Features ---\")\n",
        "print(f\"Features: {numerical_cols_for_plots}\")\n",
        "\n",
        "# Determine grid size for subplots\n",
        "n_cols_grid = 3\n",
        "n_rows_grid = (len(numerical_cols_for_plots) - 1) // n_cols_grid + 1\n",
        "\n",
        "plt.figure(figsize=(n_cols_grid * 5, n_rows_grid * 4)) # Adjust figure size as needed\n",
        "\n",
        "for i, col in enumerate(numerical_cols_for_plots):\n",
        "    plt.subplot(n_rows_grid, n_cols_grid, i + 1)\n",
        "    sns.histplot(df[col], kde=True, bins=30) # Add kernel density estimate\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8PzYBV6njMxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same numerical_cols_for_plots as above\n",
        "plt.figure(figsize=(n_cols_grid * 5, n_rows_grid * 4.5)) # Adjust figure size\n",
        "\n",
        "for i, col in enumerate(numerical_cols_for_plots):\n",
        "    plt.subplot(n_rows_grid, n_cols_grid, i + 1)\n",
        "    sns.boxplot(x=TARGET_COLUMN, y=col, data=df, palette='viridis')\n",
        "    plt.title(f'{col} vs. {TARGET_COLUMN}')\n",
        "    plt.xlabel('Damage Grade')\n",
        "    plt.ylabel(col)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mMcvJdTAj-Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_for_corr = numerical_cols_for_plots + [TARGET_COLUMN]\n",
        "\n",
        "correlation_matrix = df[cols_for_corr].corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Matrix (Numerical Features & Damage Grade)')\n",
        "plt.show()\n",
        "\n",
        "# Display correlations specifically with the target variable\n",
        "print(f\"\\n--- Correlations with Target Variable ({TARGET_COLUMN}) ---\")\n",
        "target_correlations = correlation_matrix[TARGET_COLUMN].drop(TARGET_COLUMN).sort_values(ascending=False)\n",
        "print(target_correlations)"
      ],
      "metadata": {
        "id": "GpLO2bdGl6Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Categorical Feature Overview\n",
        "print(\"\\n--- Analyzing Categorical Features ---\")\n",
        "\n",
        "# Identify categorical columns (object dtype)\n",
        "categorical_cols_obj = df.select_dtypes(include='object').columns.tolist()\n",
        "print(f\"Object type columns ({len(categorical_cols_obj)}): {categorical_cols_obj}\")\n",
        "\n",
        "# Also consider 'geo_level_id's as categorical, and binary flags\n",
        "geo_cols = [col for col in df.columns if col.startswith('geo_level')]\n",
        "binary_flag_cols = [col for col in df.columns if col.startswith('has_')] # Includes superstructure and secondary use for now\n",
        "\n",
        "# Combine for a broader list (excluding target)\n",
        "all_potential_categorical = sorted(list(set(categorical_cols_obj + geo_cols + binary_flag_cols)))\n",
        "all_potential_categorical = [col for col in all_potential_categorical if col != TARGET_COLUMN]\n",
        "\n",
        "print(f\"\\n--- Unique Value Counts for Potential Categorical Features ---\")\n",
        "# Calculate and display unique counts for each\n",
        "unique_counts = {col: df[col].nunique() for col in all_potential_categorical if col in df.columns}\n",
        "\n",
        "# Sort by number of unique values for clarity\n",
        "unique_counts_sorted = dict(sorted(unique_counts.items(), key=lambda item: item[1]))\n",
        "\n",
        "for col, count in unique_counts_sorted.items():\n",
        "    print(f\"- {col}: {count} unique values\")\n",
        "\n",
        "# Define key low-cardinality categoricals for detailed plots (exclude binary flags for now)\n",
        "key_categoricals = [\n",
        "    'land_surface_condition', 'foundation_type', 'roof_type',\n",
        "    'ground_floor_type', 'other_floor_type', 'position',\n",
        "    'plan_configuration', 'legal_ownership_status'\n",
        "]\n",
        "# Filter to only those present in the current df\n",
        "key_categoricals = [col for col in key_categoricals if col in df.columns]"
      ],
      "metadata": {
        "id": "O71RuH1AnMZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_key_cats = len(key_categoricals)\n",
        "n_cols_grid_cat = 2\n",
        "n_rows_grid_cat = (n_key_cats - 1) // n_cols_grid_cat + 1\n",
        "\n",
        "plt.figure(figsize=(n_cols_grid_cat * 7, n_rows_grid_cat * 5)) # Adjust figure size\n",
        "\n",
        "for i, col in enumerate(key_categoricals):\n",
        "    plt.subplot(n_rows_grid_cat, n_cols_grid_cat, i + 1)\n",
        "    # Order bars by the total frequency of the category\n",
        "    order = df[col].value_counts().index\n",
        "    sns.countplot(data=df, x=col, hue=TARGET_COLUMN, order=order, palette='viridis')\n",
        "    plt.title(f'{TARGET_COLUMN} Distribution by {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right') # Rotate labels if needed\n",
        "    plt.legend(title='Damage Grade')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# It's hard to plot all flags. Let's look at a few important ones vs damage.\n",
        "superstructure_flags_to_check = [\n",
        "    'has_superstructure_rc_engineered',\n",
        "    'has_superstructure_rc_non_engineered',\n",
        "    'has_superstructure_mud_mortar_stone',\n",
        "    'has_superstructure_timber',\n",
        "    'has_superstructure_adobe_mud'\n",
        "]\n",
        "\n",
        "\n",
        "print(f\"\\n--- Plotting Selected Superstructure Flags vs. {TARGET_COLUMN} ---\")\n",
        "n_flags = len(superstructure_flags_to_check)\n",
        "plt.figure(figsize=(n_flags * 4, 16))\n",
        "\n",
        "for i, flag in enumerate(superstructure_flags_to_check):\n",
        "    plt.subplot(1, n_flags, i + 1)\n",
        "    # Ensure the flag column is treated as categorical for hue grouping\n",
        "    sns.countplot(data=df, x=flag, hue=TARGET_COLUMN, palette='viridis')\n",
        "    plt.title(f'Damage by {flag}')\n",
        "    plt.xlabel(f'Has {flag.split(\"_\")[-1]}? (0=No, 1=Yes)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(title='Grade', loc='upper center') # Adjust legend position\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Analysis for Geo Level 1 ID ---\n",
        "print(f\"\\n--- Plotting {TARGET_COLUMN} Distribution by geo_level_1_id ---\")\n",
        "plt.figure(figsize=(14, 6))\n",
        "order_geo1 = df['geo_level_1_id'].value_counts().index # Order by frequency\n",
        "sns.countplot(data=df, x='geo_level_1_id', hue=TARGET_COLUMN, order=order_geo1, palette='viridis')\n",
        "plt.title(f'{TARGET_COLUMN} Distribution by geo_level_1_id')\n",
        "plt.xlabel('Geo Level 1 ID')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Damage Grade')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4tIta94boEgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- Plotting Selected Superstructure Flags vs. {TARGET_COLUMN} ---\")\n",
        "n_flags = len(superstructure_flags_to_check)\n",
        "n_cols = 2  # Number of columns in the grid (set to 2)\n",
        "n_rows = (n_flags + n_cols - 1) // n_cols  # Number of rows (calculated)\n",
        "plt.figure(figsize=(n_cols * 4, n_rows * 4))  # Adjust figure size\n",
        "\n",
        "for i, flag in enumerate(superstructure_flags_to_check):\n",
        "    plt.subplot(n_rows, n_cols, i + 1)\n",
        "    # Ensure the flag column is treated as categorical for hue grouping\n",
        "    sns.countplot(data=df, x=flag, hue=TARGET_COLUMN, palette='viridis')\n",
        "    plt.title(f'Damage by {flag}')\n",
        "    plt.xlabel(f'Has {flag.split(\"_\")[-1]}? (0=No, 1=Yes)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend(title='Grade', loc='upper center')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "00NXFXiv_rxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing**"
      ],
      "metadata": {
        "id": "ag68wIDACYd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping low impact / problematic features based on EDA\n",
        "irrelevant = [col for col in df.columns if col.startswith('has_secondary_')]\n",
        "irrelevant.append('count_families')\n",
        "irrelevant.append('legal_ownership_status')\n",
        "irrelevant.append('geo_level_3_id')\n",
        "irrelevant.append('position')\n",
        "\n",
        "df.drop(columns=irrelevant, inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "kjvqyO6uEnp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset\n",
        "y = df[TARGET_COLUMN]\n",
        "X = df.drop(columns = [TARGET_COLUMN])\n",
        "\n",
        "y = y-1 # adjusting target / damage grade to be 0 indexed\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.15,\n",
        "    random_state=13,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val,\n",
        "    test_size=0.1765,\n",
        "    random_state=17,\n",
        "    stratify=y_train_val\n",
        ")"
      ],
      "metadata": {
        "id": "9WAc85ioC2JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns for one-hot encoding\n",
        "ohe_cols = sorted(list(set(\n",
        "    X_train.select_dtypes(include='object').columns.tolist()\n",
        "    + ['geo_level_1_id' , 'geo_level_2_id'])))\n",
        "\n",
        "# Binary Passthrough Columns:\n",
        "bin_cols = sorted([\n",
        "    col for col in X_train.columns if col.startswith('has_superstructure_')\n",
        "])\n",
        "\n",
        "# Numerical Columns to Scale:\n",
        "num_cols = sorted([\n",
        "    col for col in X_train.columns if col not in ohe_cols and col not in bin_cols\n",
        "])"
      ],
      "metadata": {
        "id": "i_YYUtt7GIxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', dtype=np.int8)\n",
        "scaler.fit(X_train[num_cols])\n",
        "encoder.fit(X_train[ohe_cols])\n",
        "ohe_names = encoder.get_feature_names_out(ohe_cols)"
      ],
      "metadata": {
        "id": "o4AMtphKOmgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming Numerical Features\n",
        "X_train_num = scaler.transform(X_train[num_cols]).astype(np.float32)\n",
        "X_val_num = scaler.transform(X_val[num_cols]).astype(np.float32)\n",
        "X_test_num = scaler.transform(X_test[num_cols]).astype(np.float32)"
      ],
      "metadata": {
        "id": "NVkYXnm3Rpza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming Categorical Features\n",
        "X_train_ohe = encoder.transform(X_train[ohe_cols]) # dtype=int8 set during init\n",
        "X_val_ohe = encoder.transform(X_val[ohe_cols])\n",
        "X_test_ohe = encoder.transform(X_test[ohe_cols])"
      ],
      "metadata": {
        "id": "tSKuUgT1SQOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Passthrough Features\n",
        "X_train_bin = X_train[bin_cols].to_numpy(dtype=np.int8)\n",
        "X_val_bin = X_val[bin_cols].to_numpy(dtype=np.int8)\n",
        "X_test_bin = X_test[bin_cols].to_numpy(dtype=np.int8)"
      ],
      "metadata": {
        "id": "scQtU-lsT1Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining features into final numpy arrays\n",
        "X_train_final = np.hstack([X_train_num, X_train_bin, X_train_ohe])\n",
        "X_val_final = np.hstack([X_val_num, X_val_bin, X_val_ohe])\n",
        "X_test_final = np.hstack([X_test_num, X_test_bin, X_test_ohe])\n",
        "\n",
        "print(f\"Final combined array shape (Train): {X_train_final.shape}\")\n",
        "print(f\"Final combined array shape (Validation):   {X_val_final.shape}\")\n",
        "print(f\"Final combined array shape (Test):  {X_test_final.shape}\")\n",
        "\n",
        "\n",
        "# Deleting large intermediate arrays\n",
        "import gc\n",
        "del X_train_num, X_val_num, X_test_num\n",
        "del X_train_ohe, X_val_ohe, X_test_ohe\n",
        "del X_train_bin, X_val_bin, X_test_bin\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "gZ3Jvo00UG8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming y as well to numpy for efficiency\n",
        "y_train_final = y_train.to_numpy()\n",
        "y_val_final = y_val.to_numpy()\n",
        "y_test_final = y_test.to_numpy()\n",
        "\n",
        "print(f\"Final target array shape (Train): {y_train_final.shape}\")\n",
        "print(f\"Final target array shape (Val):   {y_val_final.shape}\")\n",
        "print(f\"Final target array shape (Test):  {y_test_final.shape}\")\n",
        "print(f\"Target arrays dtype: {y_train_final.dtype}\")\n",
        "print(f\"Unique values in y_train_final: {np.unique(y_train_final)}\")"
      ],
      "metadata": {
        "id": "04aiHfXDVF8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Development & Training the Model**"
      ],
      "metadata": {
        "id": "0K4D_12HWmlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing any previous Keras session state\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model = Sequential(name=\"EarthquakeDamage_MLP_v1\")\n",
        "\n",
        "model.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "\n",
        "model.add(Dense(128, activation='relu', name='Hidden_Layer_1'))\n",
        "model.add(Dropout(rate=0.3, name='Dropout_1'))\n",
        "model.add(Dense(64, activation='relu', name='Hidden_Layer_2'))\n",
        "model.add(Dropout(rate=0.3, name='Dropout_2'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "# Compiling the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vJiPmSBfXah3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping function settings for efficiency\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")"
      ],
      "metadata": {
        "id": "ulDtK9grZgWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "history = model.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "6A6Vsy5xa03o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df['loss'], label='Training Loss')\n",
        "plt.plot(history_df['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_df['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch = history_df['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_df.loc[best_epoch, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_df.loc[best_epoch, 'val_accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "sYN2pLhYcVpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing and Evaluation**"
      ],
      "metadata": {
        "id": "5CsV-dJTkIve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test_final, y_test_final, verbose=1)\n",
        "print(f\"\\nTest Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs = model.predict(X_test_final)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1) # selecting the class with highes probability\n",
        "\n",
        "manual_accuracy = accuracy_score(y_test_final, y_pred_classes) # manually calculating accuracy\n",
        "print(f\"\\nManual Accuracy Check: {manual_accuracy:.4f} ({manual_accuracy*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "rCXjKuyYfVgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed Classification Metrics\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "\n",
        "print(classification_report(y_test_final, y_pred_classes, target_names=target_names))\n",
        "\n",
        "cm = confusion_matrix(y_test_final, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6cSLhVg_hhsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = cm.sum(axis=1)\n",
        "\n",
        "cm_normalized = cm.astype('float') / row_sums[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names if 'target_names' in locals() else sorted(np.unique(y_test_final)),\n",
        "            yticklabels=target_names if 'target_names' in locals() else sorted(np.unique(y_test_final)))\n",
        "plt.title('Normalized Confusion Matrix (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bsY4B_tZjyjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Refining the Model**\n",
        "\n",
        "The number of trials mentioned in the article doesn't match the number here because some sections were re-used multiple times while refinement."
      ],
      "metadata": {
        "id": "gJzO-7-xoaI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 1 Balancing Weights**"
      ],
      "metadata": {
        "id": "HXKYj6D9vSaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any previous Keras session state\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build the Sequential Model\n",
        "model_weighted = Sequential(name=\"EarthquakeDamage_MLP_Weighted\")\n",
        "model_weighted.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "\n",
        "model_weighted.add(Dense(128, activation='relu', name='Hidden_Layer_1'))\n",
        "model_weighted.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_weighted.add(Dense(64, activation='relu', name='Hidden_Layer_2'))\n",
        "model_weighted.add(Dropout(0.3, name='Dropout_2'))\n",
        "\n",
        "# Output Layer\n",
        "model_weighted.add(Dense(3, activation='softmax', name='Output_Layer'))\n",
        "\n",
        "# Compiling the Model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_weighted.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_weighted.summary()"
      ],
      "metadata": {
        "id": "hDIg_8iAoYyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = np.unique(y_train_final)\n",
        "\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_classes,\n",
        "    y=y_train_final\n",
        ")\n",
        "\n",
        "class_weights_dict = dict(zip(unique_classes, weights))\n",
        "\n",
        "# Early stopping function\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "history_weighted = model_weighted.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "le546tcvpaST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_dict"
      ],
      "metadata": {
        "id": "c_ltwlqivFVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_df_w = pd.DataFrame(history_weighted.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df_w['loss'], label='Training Loss')\n",
        "plt.plot(history_df_w['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df_w['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_df_w['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_w = history_df_w['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_w + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_df_w.loc[best_epoch_w, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_df_w.loc[best_epoch_w, 'val_accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "id": "KeW-4necsTMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_w, test_accuracy_w = model_weighted.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_weighted.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FTJMQ2Oes08F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: Dropped significantly to 67.73% (from 73.45%).\n",
        "Loss: Increased to 0.6813 (from 0.5880).\n",
        "Conclusion: Applying balanced class weights decreased the overall accuracy and increased the overall loss on the test set."
      ],
      "metadata": {
        "id": "zwPSVyrLucdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 2: Balancing weights manually**\n"
      ],
      "metadata": {
        "id": "tWIJRu92vbF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = np.unique(y_train_final)\n",
        "balanced_weights_array = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_classes,\n",
        "    y=y_train_final\n",
        ")\n",
        "\n",
        "balanced_weights_dict = dict(zip(unique_classes, balanced_weights_array))\n",
        "\n",
        "\n",
        "boost_factor = 0.7 #\n",
        "\n",
        "\n",
        "# 5. Create Manual Weight Dictionary: Iterate through each class to calculate its weight.\n",
        "manual_weights_dict = {}\n",
        "\n",
        "for class_index in unique_classes:\n",
        "\n",
        "    if class_index == 1: # majority class\n",
        "        manual_weights_dict[class_index] = 1.0\n",
        "        print(f\" - Class {class_index} (Majority): Weight = 1.0\")\n",
        "    else:\n",
        "        balanced_weight = balanced_weights_dict.get(class_index, 1.0)\n",
        "        adjusted_weight = (balanced_weight - 1)*boost_factor + 1\n",
        "        manual_weights_dict[class_index] = adjusted_weight\n",
        "        print(f\" - Class {class_index} (Minority): Balanced Weight={balanced_weight:.4f}, Adjusted Weight = {adjusted_weight:.4f}\")"
      ],
      "metadata": {
        "id": "Ozo7_KpJud3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual_weights_dict = {0:2.9,1:1,2:1.3}"
      ],
      "metadata": {
        "id": "Mq690Csy2G7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQdjTauw0c7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuilding the model\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model_manual_weights = Sequential(name=\"EarthquakeDamage_MLP_ManualWeight\")\n",
        "model_manual_weights.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_manual_weights.add(Dense(128, activation='relu', name='Hidden_Layer_1'))\n",
        "model_manual_weights.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_manual_weights.add(Dense(64, activation='relu', name='Hidden_Layer_2'))\n",
        "model_manual_weights.add(Dropout(0.3, name='Dropout_2'))\n",
        "model_manual_weights.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_manual_weights.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_manual_weights.summary()"
      ],
      "metadata": {
        "id": "uWkDlp_EzInz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with manually weighted balance factors\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "history_manual_weights = model_manual_weights.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "NKoGi4T2zekP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_df_mw = pd.DataFrame(history_manual_weights.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df_mw['loss'], label='Training Loss')\n",
        "plt.plot(history_df_mw['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df_mw['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_df_mw['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_mw = history_df_mw['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_mw + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_df_mw.loc[best_epoch_mw, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_df_mw.loc[best_epoch_mw, 'val_accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "id": "b7RFHs3wz6lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_w, test_accuracy_w = model_manual_weights.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_manual_weights.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iwh3ni9x0oej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 3: Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "gbyjDI155HIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuilding the model\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model_hyper = Sequential(name=\"EarthquakeDamage_MLP_ManualWeight\")\n",
        "model_hyper.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_hyper.add(Dense(128, activation='relu', name='Hidden_Layer_1'))\n",
        "model_hyper.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_hyper.add(Dense(64, activation='relu', name='Hidden_Layer_2'))\n",
        "model_hyper.add(Dropout(0.3, name='Dropout_2'))\n",
        "model_hyper.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_hyper.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_hyper.summary()"
      ],
      "metadata": {
        "id": "EjINOrvX5G6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with manually weighted balance factors\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=6,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "history_hyper = model_hyper.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "Js9R2LIC5Lvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_hyper = pd.DataFrame(history_hyper.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_hyper['loss'], label='Training Loss')\n",
        "plt.plot(history_hyper['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_hyper['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_hyper['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_hyper = history_hyper['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_hyper + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_hyper.loc[best_epoch_hyper, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_hyper.loc[best_epoch_hyper, 'val_accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "id": "xZTrCqVY698i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_w, test_accuracy_w = model_hyper.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_hyper.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "su2gyysq7VO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 4: Structure of Hidden Layers**"
      ],
      "metadata": {
        "id": "xgw3hW2lfyld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model_structure = Sequential(name=\"EarthquakeDamage_MLP_Structure\")\n",
        "model_structure.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_1'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_2'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_2'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_3'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_3'))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_4'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_4'))\n",
        "model_structure.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_structure.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_structure.summary()"
      ],
      "metadata": {
        "id": "4LQhVRbtf7Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer(model_structure, file_name=\"visual\", file_format='png')\n",
        "Image(\"visual.png\")"
      ],
      "metadata": {
        "id": "sU3QIzj5jBsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structure_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model\n",
        "history_structure = model_structure.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[structure_early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "QuOOuQDihdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_structure = pd.DataFrame(history_structure.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_structure['loss'], label='Training Loss')\n",
        "plt.plot(history_structure['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_structure['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_structure['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_structure = history_structure['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_structure + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_structure.loc[best_epoch_structure, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_structure.loc[best_epoch_structure, 'val_accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "id": "lXcIetA9heGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_w, test_accuracy_w = model_structure.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_structure.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yVSz3njdhjCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 5: Structure of Droput Layers**"
      ],
      "metadata": {
        "id": "9G4_Y4lllHGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model_structure = Sequential(name=\"EarthquakeDamage_MLP_Structure\")\n",
        "model_structure.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_1'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_2'))\n",
        "\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_3'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_3'))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_4'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_4'))\n",
        "model_structure.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_structure.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_structure.summary()\n",
        "\n",
        "structure_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model\n",
        "history_structure = model_structure.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[structure_early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_structure = pd.DataFrame(history_structure.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_structure['loss'], label='Training Loss')\n",
        "plt.plot(history_structure['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_structure['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_structure['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_structure = history_structure['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_structure + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_structure.loc[best_epoch_structure, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_structure.loc[best_epoch_structure, 'val_accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "test_loss_w, test_accuracy_w = model_structure.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_structure.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G3VIzb4plF8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer(model_structure, file_name=\"visual\", file_format='png')\n",
        "Image(\"visual.png\")"
      ],
      "metadata": {
        "id": "3r0RxtLvlve-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 6: Structure Dropout Layers**"
      ],
      "metadata": {
        "id": "wXtudo3Mmtl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model_structure = Sequential(name=\"EarthquakeDamage_MLP_Structure\")\n",
        "model_structure.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_1'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_2'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_2'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_3'))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_4'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_4'))\n",
        "model_structure.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_structure.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_structure.summary()\n",
        "\n",
        "structure_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model\n",
        "history_structure = model_structure.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[structure_early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_structure = pd.DataFrame(history_structure.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_structure['loss'], label='Training Loss')\n",
        "plt.plot(history_structure['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_structure['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_structure['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_structure = history_structure['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_structure + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_structure.loc[best_epoch_structure, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_structure.loc[best_epoch_structure, 'val_accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "test_loss_w, test_accuracy_w = model_structure.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_structure.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9yWkmWDamzHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer(model_structure, file_name=\"visual\", file_format='png')\n",
        "Image(\"visual.png\")"
      ],
      "metadata": {
        "id": "rjjhJEibomd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 7: Structure - Dropout Layers**"
      ],
      "metadata": {
        "id": "l9IUX6RUosbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model_structure = Sequential(name=\"EarthquakeDamage_MLP_Structure\")\n",
        "model_structure.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_1'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_2'))\n",
        "\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_3'))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_4'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_4'))\n",
        "model_structure.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_structure.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_structure.summary()\n",
        "\n",
        "structure_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model\n",
        "history_structure = model_structure.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[structure_early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_structure = pd.DataFrame(history_structure.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_structure['loss'], label='Training Loss')\n",
        "plt.plot(history_structure['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_structure['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_structure['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_structure = history_structure['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_structure + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_structure.loc[best_epoch_structure, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_structure.loc[best_epoch_structure, 'val_accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "test_loss_w, test_accuracy_w = model_structure.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_structure.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "llrOiKUpo8kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer(model_structure, file_name=\"visual\", file_format='png')\n",
        "Image(\"visual.png\")"
      ],
      "metadata": {
        "id": "i2JC47XDpKAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best structure seems to leave the dropout layers in the middle."
      ],
      "metadata": {
        "id": "pr3-DqAKqEiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 8: Hidden Layer Structure**\n",
        "Increasing Complexity"
      ],
      "metadata": {
        "id": "ZYmTbIuOqWvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model_structure = Sequential(name=\"EarthquakeDamage_MLP_Structure\")\n",
        "model_structure.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_1'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_2'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_2'))\n",
        "model_structure.add(Dense(256, activation='relu', name='Hidden_Layer_3'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_3'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_4'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_4'))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_5'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_5'))\n",
        "model_structure.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_structure.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_structure.summary()\n",
        "\n",
        "structure_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model\n",
        "history_structure = model_structure.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[structure_early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_structure = pd.DataFrame(history_structure.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_structure['loss'], label='Training Loss')\n",
        "plt.plot(history_structure['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_structure['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_structure['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_structure = history_structure['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_structure + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_structure.loc[best_epoch_structure, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_structure.loc[best_epoch_structure, 'val_accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "test_loss_w, test_accuracy_w = model_structure.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_structure.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cShDIQtjqT1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer(model_structure, file_name=\"visual\", file_format='png')\n",
        "Image(\"visual.png\")"
      ],
      "metadata": {
        "id": "a0ZY8YvwquZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 9: Structure - Hidden and Droupout Layers**"
      ],
      "metadata": {
        "id": "7OKVN0eMr0u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model_structure = Sequential(name=\"EarthquakeDamage_MLP_Structure\")\n",
        "model_structure.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_1'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_2'))\n",
        "\n",
        "model_structure.add(Dense(256, activation='relu', name='Hidden_Layer_3'))\n",
        "\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_4'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_4'))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_5'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_5'))\n",
        "model_structure.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_structure.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_structure.summary()\n",
        "\n",
        "structure_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model\n",
        "history_structure = model_structure.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[structure_early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_structure = pd.DataFrame(history_structure.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_structure['loss'], label='Training Loss')\n",
        "plt.plot(history_structure['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_structure['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_structure['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_structure = history_structure['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_structure + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_structure.loc[best_epoch_structure, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_structure.loc[best_epoch_structure, 'val_accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "test_loss_w, test_accuracy_w = model_structure.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_structure.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3n1CQpHGr8PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer(model_structure, file_name=\"visual\", file_format='png')\n",
        "Image(\"visual.png\")"
      ],
      "metadata": {
        "id": "tS3FLiYor-TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Trial 10: Structure - Hidden Layers**"
      ],
      "metadata": {
        "id": "OiSAAu7rs1_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model_structure = Sequential(name=\"EarthquakeDamage_MLP_Structure\")\n",
        "model_structure.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "model_structure.add(Dense(32, activation='relu', name='Hidden_Layer_1'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_1'))\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_2'))\n",
        "\n",
        "model_structure.add(Dense(128, activation='relu', name='Hidden_Layer_3'))\n",
        "\n",
        "model_structure.add(Dense(64, activation='relu', name='Hidden_Layer_4'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_4'))\n",
        "model_structure.add(Dense(32, activation='relu', name='Hidden_Layer_5'))\n",
        "model_structure.add(Dropout(0.3, name='Dropout_5'))\n",
        "model_structure.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_structure.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_structure.summary()\n",
        "\n",
        "structure_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model\n",
        "history_structure = model_structure.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[structure_early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history_structure = pd.DataFrame(history_structure.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_structure['loss'], label='Training Loss')\n",
        "plt.plot(history_structure['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_structure['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_structure['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_structure = history_structure['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_structure + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_structure.loc[best_epoch_structure, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_structure.loc[best_epoch_structure, 'val_accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "test_loss_w, test_accuracy_w = model_structure.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = model_structure.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L2LBUpMVs-he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Best Model**"
      ],
      "metadata": {
        "id": "2ilOaa31D8nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "final_model = Sequential(name=\"EarthquakeDamage_MLP_ManualWeight\")\n",
        "final_model.add(Input(shape=(X_train_final.shape[1],), name=\"Input_Layer\"))\n",
        "final_model.add(Dense(128, activation='relu', name='Hidden_Layer_1'))\n",
        "final_model.add(Dropout(0.3, name='Dropout_1'))\n",
        "final_model.add(Dense(64, activation='relu', name='Hidden_Layer_2'))\n",
        "final_model.add(Dropout(0.3, name='Dropout_2'))\n",
        "final_model.add(Dense(len(np.unique(y_train_final)), activation='softmax', name='Output_Layer'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "final_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "final_model.summary()"
      ],
      "metadata": {
        "id": "ny9E6ZODEpGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualizer(final_model, file_name=\"visual\", file_format='png')\n",
        "Image(\"visual.png\")"
      ],
      "metadata": {
        "id": "aonrE3rPtXV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model\n",
        "history_final = final_model.fit(\n",
        "    X_train_final,\n",
        "    y_train_final,\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    validation_data=(X_val_final, y_val_final),\n",
        "    callbacks=[final_early_stopping],\n",
        "    class_weight=manual_weights_dict,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "A-nxJWsZEDrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_final = pd.DataFrame(history_final.history)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#  Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_final['loss'], label='Training Loss')\n",
        "plt.plot(history_final['val_loss'], label='Validation Loss')\n",
        "plt.title('Weighted Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (Sparse Categorical Crossentropy)')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Pot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_final['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_final['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Weighted Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find epoch with best validation loss\n",
        "best_epoch_final = history_final['val_loss'].idxmin()\n",
        "print(f\"\\nBest Validation Loss achieved at epoch {best_epoch_final + 1}\")\n",
        "print(f\" - Best Validation Loss: {history_final.loc[best_epoch_final, 'val_loss']:.4f}\")\n",
        "print(f\" - Validation Accuracy at Best Epoch: {history_final.loc[best_epoch_final, 'val_accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "id": "23FW1v_GFiwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_w, test_accuracy_w = final_model.evaluate(X_test_final, y_test_final, verbose=0)\n",
        "\n",
        "print(f\"\\nWeighted Test Set Evaluation:\")\n",
        "print(f\" - Test Loss: {test_loss_w:.4f}\")\n",
        "print(f\" - Test Accuracy: {test_accuracy_w:.4f} ({test_accuracy_w*100:.2f}%)\")\n",
        "\n",
        "y_pred_probs_w = final_model.predict(X_test_final)\n",
        "y_pred_classes_w = np.argmax(y_pred_probs_w, axis=1)\n",
        "\n",
        "\n",
        "target_names = ['Grade 1', 'Grade 2', 'Grade 3']\n",
        "print(classification_report(y_test_final, y_pred_classes_w, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_w = confusion_matrix(y_test_final, y_pred_classes_w)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_w, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Weighted Model Test Set')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "\n",
        "row_sums_w = cm_w.sum(axis=1)\n",
        "cm_normalized_w = cm_w.astype('float') / row_sums_w[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_normalized_w, annot=True, fmt='.2%', cmap='Blues', # Format as percentage\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Normalized Confusion Matrix - Weighted Model (% of Actual Class)')\n",
        "plt.ylabel('Actual Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FvN0lPuUHpHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}